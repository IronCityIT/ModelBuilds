{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from transformers import TFAutoModelForCausalLM, AutoTokenizer, TFAutoModelForTokenClassification\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "\n",
        "# Hardcoded API Keys\n",
        "HF_API_KEY = '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'\n",
        "GEMINI_API = 'AIzaSyD4QKKKgfZ1Zn5o2XOSjpTEZeV4-q6CsVA'\n",
        "GOOGLE_API_KEY = 'AIzaSyD4QKKKgfZ1Zn5o2XOSjpTEZeV4-q6CsVA'\n",
        "CSE_ID = '517ac067ac2ed48a5'\n",
        "\n",
        "# Load GPT-J model and tokenizer for TensorFlow\n",
        "content_model_name = \"EleutherAI/gpt-j-6B\"\n",
        "content_tokenizer = AutoTokenizer.from_pretrained(content_model_name, token=HF_API_KEY)\n",
        "content_model = TFAutoModelForCausalLM.from_pretrained(content_model_name, token=HF_API_KEY)\n",
        "\n",
        "# Load BERT NER model and tokenizer for TensorFlow\n",
        "ner_model_name = \"dslim/bert-base-NER\"\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name, token=HF_API_KEY)\n",
        "ner_model = TFAutoModelForTokenClassification.from_pretrained(ner_model_name, token=HF_API_KEY)\n",
        "\n",
        "# GitHub repository URL\n",
        "repo_url = \"git@github.com:IronCityIT/ModelBuilds.git\"\n",
        "\n",
        "# Function to run shell commands\n",
        "def run_shell_command(command):\n",
        "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    out, err = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(f\"Error running command: {command}\\nError: {err.decode('utf-8')}\")\n",
        "    else:\n",
        "        print(out.decode('utf-8'))\n",
        "\n",
        "# Function to generate content using GPT-J and TensorFlow\n",
        "def generate_content(prompt):\n",
        "    inputs = content_tokenizer(prompt, return_tensors=\"tf\")\n",
        "    output = content_model.generate(**inputs, max_length=500)\n",
        "    generated_text = content_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Function to extract keywords using BERT NER and TensorFlow\n",
        "def extract_keywords(text):\n",
        "    inputs = ner_tokenizer(text, return_tensors=\"tf\", truncation=True, max_length=512)\n",
        "    outputs = ner_model(**inputs)\n",
        "    predictions = tf.argmax(outputs.logits, axis=-1)\n",
        "    tokens = ner_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    keywords = [token for token, prediction in zip(tokens, predictions[0]) if prediction != 0]\n",
        "    return keywords\n",
        "\n",
        "# Function to perform Google Custom Search\n",
        "def google_search(queries):\n",
        "    results = []\n",
        "    for query in queries:\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={CSE_ID}&q={query}\"\n",
        "        response = requests.get(url).json()\n",
        "        for item in response.get('items', []):\n",
        "            results.append(f\"{item['title']}: {item['snippet']}\\nURL: {item['link']}\")\n",
        "    return results\n",
        "\n",
        "# Initialize Git repository (only done once)\n",
        "if not os.path.exists('.git'):\n",
        "    run_shell_command(\"git init\")\n",
        "    run_shell_command(f\"git remote add origin {repo_url}\")\n",
        "\n",
        "# Git Configuration (done once globally)\n",
        "run_shell_command(\"git config --global user.email 'blaukaitis@ironcityit.com'\")\n",
        "run_shell_command(\"git config --global user.name 'ironcityit'\")\n",
        "\n",
        "# Define the modules and their respective objectives, topics, and compliance references\n",
        "modules = [\n",
        "    {\n",
        "        \"title\": \"Introduction to Cybersecurity\",\n",
        "        \"objective\": \"Introduce employees to the basic concepts and importance of cybersecurity.\",\n",
        "        \"topics\": [\"What is Cybersecurity?\", \"Importance of Cybersecurity.\", \"Key Terminologies: malware, phishing, ransomware, encryption\"],\n",
        "        \"compliance\": \"SOC 2 Trust Service Criteria, NIST CSF Identify and Protect Functions, ISO 27001 security controls.\",\n",
        "        \"search_queries\": [\n",
        "            \"latest cybersecurity breaches\",\n",
        "            \"end user education for Introduction to Cybersecurity\",\n",
        "            \"elearning content for Introduction to Cybersecurity\",\n",
        "            \"cybersecurity awareness training for employees\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Password Management & Authentication\",\n",
        "        \"objective\": \"Educate on best practices for creating and managing passwords, and the importance of multi-factor authentication (MFA).\",\n",
        "        \"topics\": [\"Password Best Practices\", \"Password Managers\", \"Multi-Factor Authentication (MFA)\"],\n",
        "        \"compliance\": \"NIST SP 800-63 for digital identity guidelines, SOC 2 security controls.\",\n",
        "        \"search_queries\": [\n",
        "            \"best practices for password management and MFA\",\n",
        "            \"end user education for Password Management & Authentication\",\n",
        "            \"elearning content for Password Management & Authentication\",\n",
        "            \"password security awareness training\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Phishing Awareness & Social Engineering\",\n",
        "        \"objective\": \"Help employees recognize and defend against phishing and social engineering attacks.\",\n",
        "        \"topics\": [\"Types of Phishing: spear phishing, whaling, clone phishing\", \"Recognizing Phishing Attempts\", \"Responding to Phishing Attempts\"],\n",
        "        \"compliance\": \"NIST SP 800-53 for personnel security, ISO 27001 controls on awareness and training.\",\n",
        "        \"search_queries\": [\n",
        "            \"latest phishing attacks examples\",\n",
        "            \"end user education for Phishing Awareness & Social Engineering\",\n",
        "            \"elearning content for Phishing Awareness & Social Engineering\",\n",
        "            \"phishing prevention training\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Secure Use of Devices and Applications\",\n",
        "        \"objective\": \"Ensure that employees understand how to secure both personal and company devices.\",\n",
        "        \"topics\": [\"Device Security Best Practices\", \"Mobile Security\", \"Application Security\"],\n",
        "        \"compliance\": \"SOC 2 controls on system access and operations, NIST SP 800-171 for safeguarding CUI, ISO 27001 mobile device management.\",\n",
        "        \"search_queries\": [\n",
        "            \"mobile device security best practices\",\n",
        "            \"end user education for Secure Use of Devices and Applications\",\n",
        "            \"elearning content for Secure Use of Devices and Applications\",\n",
        "            \"device security training for employees\"\n",
        "        ]\n",
        "    },\n",
        "    # Add more modules as needed\n",
        "]\n",
        "\n",
        "# Process each module: generate content, extract keywords, perform Google search, and save results\n",
        "for module in modules:\n",
        "    print(f\"Processing module: {module['title']}\")\n",
        "\n",
        "    # Generate content based on the module's objective\n",
        "    prompt = f\"Generate educational content for {module['title']}. Objective: {module['objective']}.\"\n",
        "    generated_content = generate_content(prompt)\n",
        "\n",
        "    # Extract keywords using BERT NER\n",
        "    keywords = extract_keywords(generated_content)\n",
        "\n",
        "    # Perform Google Custom Search for real-world examples using multiple queries\n",
        "    search_results = google_search(module['search_queries'])\n",
        "\n",
        "    # Save the results to a file locally\n",
        "    module_file_path = f\"{module['title'].replace(' ', '_')}_module.txt\"\n",
        "    with open(module_file_path, \"w\") as file:\n",
        "        file.write(f\"Module Title: {module['title']}\\n\")\n",
        "        file.write(f\"Objective: {module['objective']}\\n\")\n",
        "        file.write(f\"Generated Content:\\n{generated_content}\\n\\n\")\n",
        "\n",
        "        file.write(\"Extracted Keywords:\\n\")\n",
        "        for keyword in keywords:\n",
        "            file.write(f\"- {keyword}\\n\")\n",
        "\n",
        "        file.write(\"\\nReal-World Examples from Google Search:\\n\")\n",
        "        for result in search_results:\n",
        "            file.write(f\"{result}\\n\")\n",
        "\n",
        "        file.write(f\"\\nCompliance References: {module['compliance']}\\n\")\n",
        "\n",
        "    print(f\"Module {module['title']} processed and saved to {module_file_path}.\")\n",
        "\n",
        "    # Git operations to push the changes\n",
        "    run_shell_command(f\"git add {module_file_path}\")\n",
        "    commit_message = f\"Added {module['title']} module\"\n",
        "    run_shell_command(f\"git commit -m '{commit_message}'\")\n",
        "    run_shell_command(\"git push -u origin main\")\n"
      ],
      "metadata": {
        "id": "7Obbr2F6wcHi",
        "outputId": "68ecf8d1-023f-4dab-d5dc-1b0880eba6f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Prompting Quickstart\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpOYALec6N8Z"
      },
      "source": [
        "This notebook contains examples of how to write and run your first prompts with the Gemini API."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Prompting.ipynb",
      "provenance": [],
      "gpuType": "V28"
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}